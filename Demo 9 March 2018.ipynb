{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from data_processing import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dict_filename = \"word_list.txt\"\n",
    "word_filename = \"word_list_freq.txt\"\n",
    "tweets = [\"I think i need a #theoryofadeadman intervention. Literally I've been listening to them for 2 weeks straight. #sendhelp #notreally @TOADM #sarcasm\",\n",
    "          \"You can see the love in her eyes #not :) â™¥â™¥â™¥ #sarcasm\",\n",
    "          \"@bintSaquib @bintsiddique oh your up north that's why, well here down south it was hottttt I hardly got any sleep coz of it lmaoooo\", \n",
    "          \"It seems as though things are escalating in San Diego, so please everyone stay safe #notallmuslimsarebad #PrayForPeace\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cleaning process for a set of tweets\n",
    "<img src=\"pictures/pipeline_clean_tokens.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_tweets = initial_clean(tweets, path + \"/res/demo/demo_clean_tweets.txt\", word_filename, \n",
    "                             word_file_is_dict=True, split_hashtag_method=split_hashtags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for tw, clean_tw in zip(tweets, clean_tweets):\n",
    "    print(\"%s\\n%s\\n\" % (tw, clean_tw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hashtag splitter - based on word frequencies and a convergence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_list = utils.load_dictionary(path + \"/res/\" + word_filename)\n",
    "split_hashtags2(\"#HillaryClinton\", word_list, verbose=True)\n",
    "split_hashtags2(\"#hillaryclinton\", word_list, verbose=True)\n",
    "split_hashtags2(\"#BajiraoMastani\", word_list, verbose=True)\n",
    "split_hashtags2(\"#bajiraomastani\", word_list, verbose=True)\n",
    "split_hashtags2(\"#neverknowwhatyouhave\", word_list, verbose=True)\n",
    "split_hashtags2(\"#drinkwinecauseitseasier\", word_list, verbose=True)\n",
    "split_hashtags2(\"#thingspresenterssay\", word_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load tokens and pos tags for the previously cleaned tweets\n",
    "tweets = load_file(path + \"/res/demo/tokens_demo.txt\").split(\"\\n\")\n",
    "pos_tags = load_file(path + \"/res/demo/pos_demo.txt\").split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Grammatical cleaning process for tweets\n",
    "<img src=\"pictures/pipeline_gramm_tokens.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gramm_tweets = grammatical_clean(tweets, pos_tags, path + \"/res/\" + dict_filename, path + '/res/demo/demo_gramm_tweets.txt', \n",
    "                                 translate_emojis=False, replace_slang=False, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for clean_tw, gramm_tw in zip(clean_tweets, gramm_tweets):\n",
    "    print(\"%s\\n%s\\n\" % (clean_tw, gramm_tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "finest_gramm_tweets = grammatical_clean(tweets, pos_tags, path + \"/res/\" + dict_filename, path + '/res/demo/demo_finest_gramm_tweets.txt',\n",
    "                                 translate_emojis=True, replace_slang=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for clean_tw, gramm_tw in zip(clean_tweets, finest_gramm_tweets):\n",
    "    print(\"%s\\n%s\\n\" % (clean_tw, gramm_tw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Strict cleaning process for tweets\n",
    "<img src=\"pictures/pipeline_strict_tokens.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "strict_tweets = strict_clean(tweets, path + '/res/demo/demo_strict_tweets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for tw, strict_tw in zip(tweets, strict_tweets):\n",
    "    print(\"%s\\n%s\\n\" % (tw, strict_tw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Datasets\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th rowspan=\"2\">Corpus</th>\n",
    "    <th colspan=\"2\">Train Set</th> \n",
    "    <th colspan=\"2\">Test Set</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Sarcastic</th>\n",
    "    <th>Non-sarcastic</th>\n",
    "    <th>Sarcastic</th>\n",
    "    <th>Non-sarcastic</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ghosh</td>\n",
    "    <td>24453</td> \n",
    "    <td>26736</td>\n",
    "    <td>1419</td>\n",
    "    <td>2323</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Riloff</td>\n",
    "    <td>215</td> \n",
    "    <td>1153</td>\n",
    "    <td>93</td>\n",
    "    <td>495</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>SarcasmDetector</td>\n",
    "    <td>26739</td> \n",
    "    <td>167235</td>\n",
    "    <td>2971</td>\n",
    "    <td>18582</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ptacek</td>\n",
    "    <td>9200</td> \n",
    "    <td>5140</td>\n",
    "    <td>2300</td>\n",
    "    <td>1285</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "#### Pragmatic Features\n",
    "\n",
    "- tweet length in characters\n",
    "- tweet length in tokens\n",
    "- average token size\n",
    "- count of capitalized words\n",
    "- count of user-specific markers (punctuation, hashtags, user mentions, emojis and laughter)\n",
    "- count of intensifiers (strong affirmatives, negations, interjections)\n",
    "- total of 6 pragmatic features\n",
    "\n",
    "#### Word uni-grams\n",
    "- Word Unigrams (5718 features) on a heavily filtered vocabulary\n",
    "- memory requirements too high to afford higher n-grams than unigrams\n",
    "\n",
    "#### POS n-grams\n",
    "- based on CMU Twitter Part-of-Speech Tagger \n",
    "- POS Unigrams (25 features)\n",
    "- POS Bigrams (483 features)\n",
    "- POS Trigrams (3515 features) \n",
    "\n",
    "#### Sentiment\n",
    "- underlying sentiment of emojis\n",
    "- underlying sentiment of words\n",
    "- subjectivity of words (weak/strong)\n",
    "- total number of words with underlying sentiment\n",
    "- Sentiment intensity analyser for the whole tweet (Vader score)\n",
    "- SentiWordNet average score for positive, negative and neutral sentiment words\n",
    "- total of 16 sentiment features\n",
    "\n",
    "#### Topics\n",
    "- based on the train set, obtain a number of topics by training an LDA model\n",
    "- a feature = the probability of a topic in the current tweet (n topics â†’ n features)\n",
    "- for a new tweet: the previously trained LDA model is loaded, the tweet is converted to doc2bow, the distributions of the topics are predicted and used as features \n",
    "\n",
    "<img src=\"pictures/topic_distribution_just_nouns.png\">\n",
    "\n",
    "<img src=\"pictures/topics_top10words.png\">\n",
    "\n",
    "#### Similarity-measure between words\n",
    "- two features: the biggest and the smallest cosine similarity scores\n",
    "- corresponding to the most similar and most dissimilar pairs of words\n",
    "\n",
    "<img src=\"pictures/cosine_sim.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Pipeline\n",
    "\n",
    "<img src=\"pictures/training_pipeline.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "word2vec_map = utils.load_vectors(filename='glove.6B.%dd.txt' % embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the tokens, pos tags and labels for the train and test set in the specified dataset\n",
    "train_tokens, train_pos, train_labels, test_tokens, test_pos, test_labels = get_dataset(dataset=\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ml_models_demo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_features, test_features = process_features(train_tokens, train_pos, test_tokens, test_pos, word2vec_map, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# From left to right, set to true if you want the feature to be active:\n",
    "# [Pragmatic, Lexical-grams, POS-grams, Sentiment, LDA topics, Similarity]\n",
    "feature_options = [[False, False, True, True, True, True], [False, False, False, True, False, True], \n",
    "                   [False, True, False, False, False, False], [False, False, True, True, False, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the models\n",
    "ml_model(train_features, test_features, train_labels, test_labels, feature_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Results\n",
    "\n",
    "<br>\n",
    "<font size=\"3\" color=\"red\">\n",
    "- have a picture here with the results, highlighting the best/worst results<br>\n",
    "- have here a picture/ plot/ graph on how you compare to other researchers/studies<br>\n",
    "- have smth like mine vs. thier<br>\n",
    "</font>\n",
    "<img src=\"pictures/ml_analysis.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word embeddings - GLoVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Setting for the embeddings\n",
    "init_unk = True\n",
    "var = None\n",
    "weighted = False\n",
    "\n",
    "# Make all words lower-case for word embeddings\n",
    "x_train = [t.lower() for t in train_tokens]\n",
    "x_test = [t.lower() for t in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get word embeddings for the train and test sets\n",
    "x_train_word_emb = utils.get_tweets_embeddings(x_train, word2vec_map, embedding_dim, \n",
    "                                               init_unk=init_unk, variance=var, weighted_average=weighted)\n",
    "x_test_word_emb = utils.get_tweets_embeddings(x_test, word2vec_map, embedding_dim,\n",
    "                                              init_unk=init_unk, variance=var, weighted_average=weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifiers.linear_svm(x_train_word_emb, train_labels, x_test_word_emb, test_labels, class_ratio='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Emoji embeddings - emoji2vec\n",
    "<img src=\"pictures/emoji2vec.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "emoji2vec_map = utils.load_vectors(filename='emoji_embeddings_%dd.txt' % embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "utils.complete_analogy('ðŸ‘ª', 'ðŸ‘¦', 'ðŸ‘§', emoji2vec_map)\n",
    "utils.complete_analogy('ðŸ‘‘', 'ðŸš¹', 'ðŸšº', emoji2vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## DeepMoji - predicting emojis\n",
    "<img src=\"pictures/deepmojis.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load predicted emojis for each tweet\n",
    "x_train_emojis = utils.get_demo_emojis(\"train.txt\", x_train)\n",
    "x_test_emojis = utils.get_demo_emojis(\"test.txt\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the emoji embeddings for the train and test sets\n",
    "x_train_emoji_emb = utils.get_tweets_embeddings(x_train_emojis, emoji2vec_map, embedding_dim,\n",
    "                                                init_unk=init_unk, variance=var, weighted_average=weighted)\n",
    "x_test_emoji_emb = utils.get_tweets_embeddings(x_test_emojis, emoji2vec_map, embedding_dim,\n",
    "                                               init_unk=init_unk, variance=var, weighted_average=weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifiers.linear_svm(x_train_emoji_emb, train_labels, x_test_emoji_emb, test_labels, class_ratio='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word + Emoji embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Obtain features by concatenating word embeddings with all emoji embeddings\n",
    "x_train_features_concat = []\n",
    "for t, e in zip(x_train_word_emb, x_train_emoji_emb):\n",
    "    x_train_features_concat.append(np.concatenate((t, e), axis=0))\n",
    "print(\"\\nShape of concatenated train features: \", np.array(x_train_features_concat).shape)\n",
    "\n",
    "x_test_features_concat = []\n",
    "for t, e in zip(x_test_word_emb, x_test_emoji_emb):\n",
    "    x_test_features_concat.append(np.concatenate((t, e), axis=0))    \n",
    "print(\"\\nShape of concatenated test features: \", np.array(x_test_features_concat).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "classifiers.linear_svm(x_train_features_concat, train_labels, x_test_features_concat, test_labels, class_ratio='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"pictures/embedding_analysis.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Comparison with BoW\n",
    "\n",
    "<img src=\"pictures/bow_analysis.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network instead of SVM\n",
    "\n",
    "<img src=\"pictures/embeddings_dnn_boxplot.png\">\n",
    "\n",
    "<img src=\"pictures/bow_nn_boxplot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are these decisions based on words actually taken?\n",
    "\n",
    "- are the key features in labeling taken by identifying sarcastic words?\n",
    "\n",
    "- for each word, calculate the number of times it belongs to a sarcastic tweet (ns)\n",
    "- for each word, calculate the number of times it belongs to a regular tweet (nr)\n",
    "- for each word, calculate its relative frequence of occurance (nf âˆŠ [-1, 1])\n",
    "\n",
    "$$nf = \\frac{ns + nr}{nf - nr}$$\n",
    "\n",
    "- to classify a tweet, add the individual relative frequency for each word\n",
    "- if the sum is greater than 0, predict it as sarcastic\n",
    "- if the sum is less than 0, predict it as regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bag_of_words import rule_based_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rule_based_comparison(x_train, train_labels, x_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule_based_comparison(x_train_emojis, train_labels, x_test_emojis, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "### Embedding Layers\n",
    "- can learn embeddings based on my corpus\n",
    "- or can use pre-trained embeddings\n",
    "\n",
    "<img src=\"pictures/emb_layer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "<img src=\"pictures/lstm.png\">\n",
    "<img src=\"pictures/lstm_sequence.png\">\n",
    "<img src=\"pictures/lstm sequence classifier.png\">\n",
    "<img src=\"pictures/rnn_cell_backpropagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load LSTM model\n",
    "from IPython.display import HTML\n",
    "from data_prep_for_visualization import *\n",
    "model, index_to_word, x_test = train_lstm_for_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization of the LSTM hidden units\n",
    "# Other nice test examples: 935, 996, 1022, 1063, 2118, 3473\n",
    "one_tweet_visualization(model, x_test, index_to_word, tweet_number=2118, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(filename= path + '/plots/html_visualizations/lstm_layer_vis_0.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Does the LSTM-based model take into account the word order? And how are the embeddings affecting the LSTM-based model?\n",
    "\n",
    "<img src=\"pictures/lstm_results_ghosh.png\">\n",
    "\n",
    "<img src=\"pictures/lstm_results_all.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM with an Attention Mechanism\n",
    "<img src=\"pictures/attention_mechanism.png\">\n",
    "<img src=\"pictures/attention_model.png\">\n",
    "<img src=\"pictures/attention_results_ghosh.png\">\n",
    "<img src=\"pictures/attention_results_all.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/attention_results.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualization of the attention mechanism on clean data\n",
    "HTML(filename= path + '/plots/html_visualizations/attention_vis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualization of the attention mechanism on grammatical data\n",
    "# HTML(filename= path + '/plots/html_visualizations/attention_vis_grammatical.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing my results with previous studies (same datasets, different approaches)\n",
    "\n",
    "#### Ghosh et al.\n",
    "- paper: \"Fracking Sarcasm using Neural Network\" (2016)\n",
    "- best f-score using recursive SVMs, usign BOW + POS: 0.663\n",
    "- best f-score using recursive SVMs, usign BOW + POS + Sentiment: 0.691\n",
    "- best f-score using recursive SVMs, usign BOW + POS + Sentiment + HT-splitter: 0.732\n",
    "- best f-score for CNN + LSTM + DNN: 0.921\n",
    "- best f-score for LSTM + LSTM: 0.879\n",
    "\n",
    "#### Riloff et al.\n",
    "- paper: \"Sarcasm as Contrast between a Positive Sentiment and Negative Situation\" (2013)\n",
    "- best f-score was 0.51, using contrast(+VPs, â€“Situations), ordered & contrast(+Preds, â€“Situations)\n",
    "\n",
    "#### SarcasmDetector (author is Mathieu Cliche, 2014)\n",
    "- www.thesarcasmdetector.com\n",
    "- obtained f-scores in range 0.50 - 0.55 (SVM-based model, used sentiment, topics and n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "d = {'Ghosh': [0.66, 0.75, 0.83], 'Riloff': [0.83, 0.84, 0.98], 'SarDet': [0.67, 0.87, 0.90]}\n",
    "df = pd.DataFrame(data=d, index=[\"SVM\", \"LSTM\", \"Attention\"])\n",
    "ax.get_xaxis().set_visible(False)\n",
    "df.plot(table=True, ax=ax, figsize=(12, 8), linewidth=5, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
